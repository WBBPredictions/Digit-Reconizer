\documentclass[11pt, oneside]{article}   	% use "amsart" instead of "article" for AMSLaTeX format
\usepackage{geometry}                		% See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   		% ... or a4paper or a5paper or ... 
%\geometry{landscape}                		% Activate for rotated page geometry
%\usepackage[parfill]{parskip}    		% Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}				% Use pdf, png, jpg, or epsÂ§ with pdflatex; use eps in DVI mode
								% TeX will automatically convert eps --> pdf in pdflatex		
\usepackage{amssymb}

%SetFonts

%SetFonts


\title{Digit Recognizer}
\author{Terris Becker, Travis Barton}
%\date{}							% Activate to display a given date or no date

\begin{document}
\maketitle
\section{Abstract}
In machine learning, it is known that many algorithms working in tandem are often more effective than one on its own. In classification this is particularly true. When deciding which category a new piece of data will fall into, (eg: converting handwritten digits for computers) the aggregate decision of multiple algorithms will tend to be more accurate than trying to listen to just one. This was the problem that we have tackled. Through a combination of Support Vector Machines, K-Nearest Neighbor algorithms, and Random Forest algorithms, we are able to create a new process that achieved a success rate of  \% , landing us in the top    of competitors (CITE THE KAGGLE CHALLENGE). This combination of machine learning algorithms forms a structure named the BeckerBarton Device (BBD). The BBD implements redundancies in the form of traditional categorical regression models in order to further verify that the correct decision has been made.

\section{Data Description}
The data comes from the Kaggle 'Digit Recognizer' data challenge (CITE), which describes each observational unit as a picture of a handwritten digit "28 pixels in height and 28 pixels in width, for a total of 784 pixels in total. Each pixel has a single pixel-value associated with it, indicating the lightness or darkness of that pixel, with higher numbers meaning darker. This pixel-value is an integer between 0 and 255, inclusive." Since our data was in grey scale, we did not need to be concerned with the color of the digit, only with the shape.
Since our algorithms do not care if the data is structured in a matrix or not, each observation will take the form of a row with 784 variables (one for each pixel). Picture this what you get after pulling a thread at the top of each picture and unraveling it pixel by pixel. The unraveled 'thread' is what the BBD will feed into it's algorithms.
\section{Methods}
The BBD utilizes the decision making power of 3 different machine learning techniques, Support Vector Machines, K Nearest Neighbors, and Random Forest. Each one is able to distinguish certain numbers better than others. Through repeated sampling, we were able to establish a rough empirical distribution of which numbers each technique predicts best. The heart of the BBD is the utilization of each algorithms? strengths. Before describing the algorithm itself in detail, we will give a high level summary of each method. Although these techniques can be used for regression or categorization, we will only be looking at the classification methods, as those are the only ones that pertain to our goal.
\subsection{SVM}
Support Vector Machines, or SVM, attempts to analyze data by creating m, n-dimensional hyperplanes planes that reduce the residual error as much as possible. These hyperplanes attempt to act as borders, separating the response into its categories. Since we have 10 response values (digits 0-9), we need m = 9, n-dimensional planes to separate them. The number of dimensions for each plane depends on how many predictors we use. Our 'unraveled thread? contained the information from a 28x28 pixel image, so our planes each have n = 784 dimensions. For the purposes of explanation, we will look at a 2 dimensional example: (insert figures here) If one were to attempt draw a line that separates the red dots from the blue dots, it would be impossible to do so perfectly with a strait line, no matter how it is rotated (figure 1). Luckily, SVM allows for planes to have varying degrees of 'curveyness' based on the /gamma parameter. Adjusting /gamma can allow the model to be more or less stringent, too little and the model will not perform well, too much and the model will be overfit. Tuning the parameter to the perfect amount is difficult, but can be approximated via bootstrapping/sampling techniques. Another parameter to consider is the 'cost'. When drawing the planes that separate the data, SVM first finds the different regions where each response category is centered. The space between these regions can different rules when it comes to how 'empty? they are allowed to be. When no points are allowed in between each region, the model is called 'strict?  and the cost parameter is 0. As the cost parameter is increased, more and more points are allowed to reside in the space between regions. Strict models rarely perform at their peak, with many packages defaulting the cost value to 1. After tuning our model with the linear.svc function from the sklearn package, we used a cost parameter of .01. For a more in-depth look at SVM and the underlying math check out (INSERT HERE).(CITE)
To further improve the accuracy of our SVM model, we created 100 SVM models from different randomly subsetted training sets, then decided the category of new data via a vote from our 100 different models. This is done in the hopes that diversifying our selection process will aid in classifying borderline data points, and help us identify the structure of difficult to classify numbers.
\subsection{KNN}
K nearest neighbor (KNN) is the most intuitive of the machine learning techniques. Imagine a having a group of people were everyone?s height, weight and sex is known except for one person called /sigma. The only thing known about /sigma is their height and weight, but not their sex. One way to guess at the sex of sigma, is to look at the k people with the most similar heights and weights, and make a guess based on what the most common sex amongst that group is. That is the essence of KNN, training data is fit to a model, and the response of a new data point (denoted as /sigma) is determined by the responses of the k nearest points to /sigma. Nearest, in our case, means Euclidean distance, but other metrics can be used. The points that make up the k-nearest neighbors are called the neighborhood of /sigma, and they can be processed in different ways. The most common way is a simple vote. Each point inside the neighborhood is asked what it?s response category is, and the category with the most points determines the prediction of sigma. This way, all points have an equal say in the decision, regardless of how far away they are from /sigma. We decided that the distance between the point in the neighborhood and sigma should have an impact, so our voting scheme is weighted with a gaussian curve. Thus, closer points to sigma have a vote that is worth more than points that are farther away. KNN has some advantages over SVM, one of which is that the only parameter that needs to be tuned is the number of points inside of each neighborhood, k. To find the optimal k, we used the tune function from the FNN package (CITE), resulting in a k = 4. For a more in-depth look into KNN and the underlying math behind it, check out (INSERT HERE)(CITE)
\subsection{Random Forest Algorithm}
The Random forest Algorithm is the only one of the three utilized machine learning techniques that does not take into account all of the predictors all of the time. The principal behind the Random Forest Algorithm lies in creating N decision trees and making a decision based on the winner of a vote where every tree gets an equal say. For more information on decision trees, check out (INSERT HERE)(CITE). The number of trees used in each forest is determined based on computing power and possible subsets of the predictors. Decision trees can be overfit if too many predictors are fed through them, but ignoring large swathes of predictors underutilized the information. The random forest algorithm solves this trade off by randomly subsetting M of the predictors and creating a decision tree based on those predictors, then repeating this process N times. This way, no individual tree is over fit, and all of the predictors are utilized. The number of trees that can be created in our model is bounded by 784 choose M.  It is important to note that many of the predictors in our data set were completely empty, and where thus removed prior to utilizing the model. The parameters to tune in a random forest model are the number of trees (N) and the number of predictors inside each tree (M). Using the estimators function inside the skleanr.RandomForestClassifier package, we decided on a M = 10 and an N = 100. For a more in-depth look into the Random Forest Algorithm and the underlying math behind it, check out (INSERT HERE)(CITE).

%\subsection{}



\end{document}  